{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "markov_sentences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCjcxFDWiljDZSA0jy4xnH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jben-hun/colab_notebooks/blob/master/algorithms/markov_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEw4MXYzEpwO"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDh1P8w8bjPh",
        "outputId": "3989bc42-9b89-4316-e343-042ea5c27ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install -q praw\n",
        "\n",
        "import praw\n",
        "import re\n",
        "import random\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from collections import deque\n",
        "\n",
        "pd.set_option(\"max_colwidth\", None)\n",
        "\n",
        "client_id = \"\" #@param {type:\"string\"}\n",
        "client_secret = \"\" #@param {type:\"string\"}\n",
        "user_agent = \"\" #@param {type:\"string\"}\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=client_id,\n",
        "    client_secret=client_secret,\n",
        "    user_agent=user_agent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 153kB 3.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 9.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWUQX6EAKptQ"
      },
      "source": [
        "class RedditMarkovChain:\n",
        "  def __init__(\n",
        "      self,\n",
        "      *subreddits,\n",
        "      sentence_limit=1000,\n",
        "      begin_str = \"*BEGIN*\",\n",
        "      end_str = \"*END*\",\n",
        "      cycle_str = \"*CYCLE*\",\n",
        "      train_split = (0.9)):\n",
        "    self.__subreddits = subreddits\n",
        "    self.sentence_limit = sentence_limit\n",
        "    self.begin_str = begin_str\n",
        "    self.end_str = end_str\n",
        "    self.cycle_str = cycle_str\n",
        "    self.__train_split = train_split\n",
        "    self.__test_split = (1.0 - train_split)\n",
        "\n",
        "    sentence_data = {}\n",
        "\n",
        "    for subreddit in self.subreddits:\n",
        "      sentence_data[subreddit] = self.mine_subreddit(\n",
        "          subreddit=reddit.subreddit(subreddit),\n",
        "          sentence_limit=self.sentence_limit)\n",
        "      \n",
        "    self.__sentence_data = sentence_data\n",
        "\n",
        "    self.models = self.__build_models(self.__sentence_data)\n",
        "\n",
        "\n",
        "  def __build_models(self, sentence_data):\n",
        "    \"\"\"Build markov chain models from extracted sentences\"\"\"\n",
        "    \n",
        "    models = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
        "\n",
        "    for subreddit_name, sentences in sentence_data.items():\n",
        "      for sentence in sentences[:int(self.train_split*len(sentences))]:\n",
        "        words = self.split_sentence(sentence)\n",
        "        models[subreddit_name][self.begin_str][words[0]] += 1\n",
        "        models[subreddit_name][words[-1]][self.end_str] += 1\n",
        "        for i in range(len(words) - 1):\n",
        "          models[subreddit_name][words[i]][words[i + 1]] += 1\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "  @property\n",
        "  def subreddits(self):\n",
        "    return self.__subreddits\n",
        "\n",
        "\n",
        "  @property\n",
        "  def train_split(self):\n",
        "    return self.__train_split\n",
        "\n",
        "\n",
        "  @property\n",
        "  def test_split(self):\n",
        "    return self.__test_split\n",
        "\n",
        "\n",
        "  @property\n",
        "  def sentence_data(self):\n",
        "    return self.__sentence_data\n",
        "\n",
        "\n",
        "  def generate(self, model_name, method):\n",
        "    \"\"\"Generate text using the created markov chain models\n",
        "\n",
        "    method:\n",
        "      expected: choose most likely words, infinite cycles are possible\n",
        "      random: choose words uniformly\n",
        "      sample: choose words based on the modeled probabilities\n",
        "    \"\"\"\n",
        "\n",
        "    sentence = \"\"\n",
        "    word = self.begin_str\n",
        "\n",
        "    if method == \"expected\":\n",
        "      used = set()\n",
        "\n",
        "    while True:\n",
        "      if method == \"expected\":\n",
        "        word = max(\n",
        "            self.models[model_name][word].items(), key=lambda x: x[1])[0]\n",
        "      elif method == \"random\":\n",
        "        word = random.choice(tuple(self.models[model_name][word].items()))[0]\n",
        "      elif method == \"sample\":\n",
        "        words = tuple(self.models[model_name][word].keys())\n",
        "        probs = self.get_probs(self.models[model_name][word])\n",
        "        word = np.random.choice(words, p=probs)\n",
        "      if word == self.end_str:\n",
        "        break\n",
        "      if word not in \".?!,\":\n",
        "        sentence += \" \"\n",
        "      sentence += word\n",
        "\n",
        "      if method == \"expected\":\n",
        "        if word in used:\n",
        "          sentence += f\" {self.cycle_str}\"\n",
        "          break\n",
        "        used.add(word)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "  def classify(self, sentence):\n",
        "    \"\"\"Deduce the most likely source of a sentence\"\"\"\n",
        "    result = {}\n",
        "    for subreddit_name, model in self.models.items():\n",
        "      words = self.split_sentence(sentence)\n",
        "      p = self.get_prob(model[self.begin_str], words[0])\n",
        "      for i in range(len(words)-1):\n",
        "        p *= self.get_prob(model[words[i]], words[i+1])\n",
        "      p *= self.get_prob(model[words[-1]], self.end_str)\n",
        "      result[subreddit_name] = p\n",
        "    return result\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def mine_subreddit(cls, subreddit, sentence_limit):\n",
        "    \"\"\"Extract clean sentences from submissions and comments\"\"\"\n",
        "\n",
        "    # re that matches clean sentences\n",
        "    matcher = re.compile(r\"(?:[.!?] |^)[A-Z][\\w', ]+[.!?](?= [A-Z]|$)\")\n",
        "\n",
        "    sentences = []\n",
        "    with tqdm.tqdm(total=sentence_limit) as pbar:\n",
        "      for submission in subreddit.hot(limit=None):\n",
        "        sentences += matcher.findall(submission.title)\n",
        "        sentences += matcher.findall(submission.selftext)\n",
        "\n",
        "        submission.comment_sort = \"best\"\n",
        "\n",
        "        comments = [comment.body for comment in submission.comments.list()\n",
        "                    if not isinstance(comment, praw.models.MoreComments)]\n",
        "\n",
        "        for comment in comments:\n",
        "          sentences += matcher.findall(comment)\n",
        "\n",
        "        len_sentences = len(sentences)\n",
        "        if len_sentences >= sentence_limit:\n",
        "          random.shuffle(sentences)\n",
        "          pbar.update(sentence_limit - pbar.n)\n",
        "          break\n",
        "        else:\n",
        "          pbar.update(len_sentences - pbar.n)\n",
        "    \n",
        "    return [cls.process_sentence(sentence) for sentence\n",
        "            in sentences[:sentence_limit]]\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def process_sentence(sentence):\n",
        "    \"\"\"Clean up sentences\"\"\"\n",
        "    return (sentence.lstrip(\".!? \")\n",
        "                    .replace(\"won't\", \"will not\")\n",
        "                    .replace(\"n't\", \" not\")\n",
        "                    .replace(\"'m\", \" am\")\n",
        "                    .replace(\"'re\", \" are\"))\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def split_sentence(sentence):\n",
        "    \"\"\"Split sentences into words\"\"\"\n",
        "    return re.findall(r\"((?:[\\w']+)|(?:[,!.?]))\", sentence)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def get_prob(d, word):\n",
        "    \"\"\"Get single probability from word counts\"\"\"\n",
        "    return 0 if word not in d else d[word]/sum(d.values())\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def get_probs(d):\n",
        "    \"\"\"Get all probabilities from word counts\"\"\"\n",
        "    n = sum(d.values())\n",
        "    return [v/n for v in d.values()]\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def traverse_comments(comments, *, breadth_first=False):\n",
        "    queue = deque(comments[:])\n",
        "    result = []\n",
        "    while queue:\n",
        "      e = queue.pop()\n",
        "      if isinstance(e, praw.models.MoreComments):\n",
        "        if breadth_first:\n",
        "          queue.extendleft(e.comments())\n",
        "        else:\n",
        "          queue.extend(e.comments())\n",
        "      else:\n",
        "        if breadth_first:\n",
        "          queue.extendleft(e.replies)\n",
        "        else:\n",
        "          queue.extend(e.replies)\n",
        "        result.append(e)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD3OnLbKEvfF"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGoyfE3idaXk",
        "outputId": "9bad7e9a-1ae9-4d09-ead0-8b3bcd7c2fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "rmc = RedditMarkovChain(\"explainlikeimfive\", \"askreddit\", \"dankmemes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:15<00:00, 66.17it/s]\n",
            "100%|██████████| 1000/1000 [00:11<00:00, 83.42it/s]\n",
            "100%|██████████| 1000/1000 [00:27<00:00, 36.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRVVf74Jpq7r"
      },
      "source": [
        "**Deriving most probable sentence for each model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqMPhSKlY2UG",
        "outputId": "f3db0c60-1bae-4721-8c39-c3c6fa77edf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "for subreddit_name in rmc.subreddits:\n",
        "  print(f\"{subreddit_name}: {rmc.generate(subreddit_name, 'expected')}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "explainlikeimfive:  I am not know what you are not *CYCLE*\n",
            "askreddit:  I am a lot of the same.\n",
            "dankmemes:  I am not know how to be the same as a lot of the *CYCLE*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-kULdBpY6Z"
      },
      "source": [
        "**Generating new text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDInUB-pnBSY",
        "outputId": "a47623f6-7feb-4625-b8db-7f4e50df5a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "dict_data = defaultdict(lambda: [])\n",
        "for subreddit_name in rmc.subreddits:\n",
        "  for i in range(5):\n",
        "    sentence = (rmc.generate(subreddit_name, \"sample\"))\n",
        "    dict_data[\"sentence\"].append(sentence)\n",
        "    dict_data[\"model\"].append(subreddit_name)\n",
        "    res = rmc.classify(sentence)\n",
        "    for k, v in res.items():\n",
        "      dict_data[f\"P({k})\"].append(v)\n",
        "display(pd.DataFrame(dict_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>model</th>\n",
              "      <th>P(explainlikeimfive)</th>\n",
              "      <th>P(askreddit)</th>\n",
              "      <th>P(dankmemes)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>That map.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>2.469136e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Terminal velocity, but they the top and answers with boiling, featuring the heat food not a good answer I am all be said, but you are they needed to do astronomers really see.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>3.461568e-48</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A huge portion of historical baggage.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>1.722653e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You can be able to stop the center point.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>1.854271e-10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Picking scabs.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>5.555556e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Once I wish I heard this is gone, Jews pray for 10 seashells each bring back.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.364019e-19</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Just beware of the flip he was and office.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.125643e-12</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Regulators!</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.111111e-03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Sad to win the coup failed to begin with a lot of equipment from paying job and maintain power.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.284713e-22</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Sometimes parents.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.968254e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Same, because he never sees me that an easy source for believing in England or two or both wash clothes is just let me in.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.914631e-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Yeah!</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.111111e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Came here.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.555556e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Tyler1, open mouthing.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.469136e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>It's more!</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.169591e-04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                            sentence  ...  P(dankmemes)\n",
              "0                                                                                                                                                                          That map.  ...  0.000000e+00\n",
              "1    Terminal velocity, but they the top and answers with boiling, featuring the heat food not a good answer I am all be said, but you are they needed to do astronomers really see.  ...  0.000000e+00\n",
              "2                                                                                                                                              A huge portion of historical baggage.  ...  0.000000e+00\n",
              "3                                                                                                                                          You can be able to stop the center point.  ...  0.000000e+00\n",
              "4                                                                                                                                                                     Picking scabs.  ...  0.000000e+00\n",
              "5                                                                                                      Once I wish I heard this is gone, Jews pray for 10 seashells each bring back.  ...  0.000000e+00\n",
              "6                                                                                                                                         Just beware of the flip he was and office.  ...  0.000000e+00\n",
              "7                                                                                                                                                                        Regulators!  ...  0.000000e+00\n",
              "8                                                                                    Sad to win the coup failed to begin with a lot of equipment from paying job and maintain power.  ...  0.000000e+00\n",
              "9                                                                                                                                                                 Sometimes parents.  ...  0.000000e+00\n",
              "10                                                        Same, because he never sees me that an easy source for believing in England or two or both wash clothes is just let me in.  ...  1.914631e-29\n",
              "11                                                                                                                                                                             Yeah!  ...  1.111111e-03\n",
              "12                                                                                                                                                                        Came here.  ...  5.555556e-04\n",
              "13                                                                                                                                                            Tyler1, open mouthing.  ...  2.469136e-06\n",
              "14                                                                                                                                                                        It's more!  ...  1.169591e-04\n",
              "\n",
              "[15 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMyrKXKJpl_J"
      },
      "source": [
        "**Classifying real text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J92CBICZnTrg",
        "outputId": "013aef81-7e56-4ee9-af1a-cf4bbf8b6315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "dict_data = defaultdict(lambda: [])\n",
        "for subreddit_name, sentences in rmc.sentence_data.items():\n",
        "  for i in tuple(range(int(rmc.test_split*len(sentences))))[:5]:\n",
        "    sentence = sentences[int(rmc.train_split*len(sentences)) + i]\n",
        "    dict_data[\"sentence\"].append(sentence)\n",
        "    dict_data[\"source\"].append(subreddit_name)\n",
        "    res = rmc.classify(sentence)\n",
        "    for k, v in res.items():\n",
        "      dict_data[f\"P({k})\"].append(v)\n",
        "display(pd.DataFrame(dict_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>source</th>\n",
              "      <th>P(explainlikeimfive)</th>\n",
              "      <th>P(askreddit)</th>\n",
              "      <th>P(dankmemes)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Take a hose and run some water through it.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Contact area, air convection, the type of meat are all important.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>7.146701e-19</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Op seems happy with that answer, but why is 400F optimal in a physics sense?</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Maillard reaction is a big swath of reactions but, basically, you need to be hot enough for stuff to start breaking down and reacting but not so hot that the carbohydrate completely breaks down to carbon.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ca not do it super accurately but that 77ft wave was measured from the video with some AI algorithms.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>When it was time to get going I woke everyone and told them we were heading out in 30 minutes.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Yes I am a conservative.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>She was an amazing woman.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.772107e-07</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>To her and the rest of the royal family in France, they only had their people's interests at heart and it was more that she was hurt and confused that they could not see that.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>To pick up the pieces and help the citizenry rebuild rather than leaving the ruins to smolder?</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Seeing the number of downvotes I got, people really do not know what dank means tho.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I have proven that I have more power than shaggy, I am the one above all.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>You do realise the meme is not made for where you live?</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Youre useless, buddy.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>MUDA !</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                        sentence  ... P(dankmemes)\n",
              "0                                                                                                                                                                     Take a hose and run some water through it.  ...         0.00\n",
              "1                                                                                                                                              Contact area, air convection, the type of meat are all important.  ...         0.00\n",
              "2                                                                                                                                   Op seems happy with that answer, but why is 400F optimal in a physics sense?  ...         0.00\n",
              "3   Maillard reaction is a big swath of reactions but, basically, you need to be hot enough for stuff to start breaking down and reacting but not so hot that the carbohydrate completely breaks down to carbon.  ...         0.00\n",
              "4                                                                                                          Ca not do it super accurately but that 77ft wave was measured from the video with some AI algorithms.  ...         0.00\n",
              "5                                                                                                                 When it was time to get going I woke everyone and told them we were heading out in 30 minutes.  ...         0.00\n",
              "6                                                                                                                                                                                       Yes I am a conservative.  ...         0.00\n",
              "7                                                                                                                                                                                      She was an amazing woman.  ...         0.00\n",
              "8                                To her and the rest of the royal family in France, they only had their people's interests at heart and it was more that she was hurt and confused that they could not see that.  ...         0.00\n",
              "9                                                                                                                 To pick up the pieces and help the citizenry rebuild rather than leaving the ruins to smolder?  ...         0.00\n",
              "10                                                                                                                          Seeing the number of downvotes I got, people really do not know what dank means tho.  ...         0.00\n",
              "11                                                                                                                                     I have proven that I have more power than shaggy, I am the one above all.  ...         0.00\n",
              "12                                                                                                                                                       You do realise the meme is not made for where you live?  ...         0.00\n",
              "13                                                                                                                                                                                         Youre useless, buddy.  ...         0.00\n",
              "14                                                                                                                                                                                                        MUDA !  ...         0.01\n",
              "\n",
              "[15 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqmvWFGHFEY9"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AaPLFDCFFun"
      },
      "source": [
        "*   Second order markov chains: P(AB->C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqPVLY7jEIW1"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvTiyWstD104"
      },
      "source": [
        "*   https://en.wikipedia.org/wiki/Markov_chain\n",
        "*   https://www.reddit.com/r/SubredditSimulator/comments/3g9ioz/what_is_rsubredditsimulator/\n",
        "*   https://www.reddit.com/r/SubSimulatorGPT2/comments/btfhks/what_is_rsubsimulatorgpt2/"
      ]
    }
  ]
}
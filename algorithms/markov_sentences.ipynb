{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "markov_sentences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1EWKzb3wTmC4G91STN/AU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jben-hun/colab_notebooks/blob/master/algorithms/markov_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEw4MXYzEpwO",
        "colab_type": "text"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDh1P8w8bjPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "13e6d282-7138-443a-c6e4-dc8499b5152b"
      },
      "source": [
        "!pip install -q praw\n",
        "\n",
        "import praw\n",
        "import re\n",
        "import random\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from collections import deque\n",
        "\n",
        "pd.set_option(\"max_colwidth\", None)\n",
        "\n",
        "client_id = \"\" #@param {type:\"string\"}\n",
        "client_secret = \"\" #@param {type:\"string\"}\n",
        "user_agent = \"\" #@param {type:\"string\"}\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=client_id,\n",
        "    client_secret=client_secret,\n",
        "    user_agent=user_agent)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 153kB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 4.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWUQX6EAKptQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SUBREDDITS = (\"explainlikeimfive\", \"askreddit\", \"dankmemes\")\n",
        "SENTENCE_LIMIT = 1000\n",
        "BEGIN_STR = \"*BEGIN*\"\n",
        "END_STR = \"*END*\"\n",
        "CYCLE_STR = \"*CYCLE*\"\n",
        "TRAIN_SPLIT = (0.9)\n",
        "TEST_SPLIT = (1.0 - TRAIN_SPLIT)\n",
        "\n",
        "\n",
        "def process_sentence(sentence):\n",
        "  \"\"\"Clean up sentences\"\"\"\n",
        "  return (sentence.lstrip(\".!? \")\n",
        "                  .replace(\"won't\", \"will not\")\n",
        "                  .replace(\"n't\", \" not\")\n",
        "                  .replace(\"'m\", \" am\")\n",
        "                  .replace(\"'re\", \" are\"))\n",
        "\n",
        "\n",
        "def split_sentence(sentence):\n",
        "  \"\"\"Split sentences into words\"\"\"\n",
        "  return re.findall(r\"((?:[\\w']+)|(?:[,!.?]))\", sentence)\n",
        "\n",
        "\n",
        "def mine_subreddit(subreddit, sentence_limit):\n",
        "  \"\"\"Extract clean sentences from submissions and comments\"\"\"\n",
        "\n",
        "  # re that matches clean sentences\n",
        "  matcher = re.compile(r\"(?:[.!?] |^)[A-Z][\\w', ]+[.!?](?= [A-Z]|$)\")\n",
        "\n",
        "  sentences = []\n",
        "  with tqdm.tqdm(total=sentence_limit) as pbar:\n",
        "    for submission in subreddit.hot(limit=None):\n",
        "      sentences += matcher.findall(submission.title)\n",
        "      sentences += matcher.findall(submission.selftext)\n",
        "\n",
        "      submission.comment_sort = \"best\"\n",
        "\n",
        "      comments = [comment.body for comment in submission.comments.list()\n",
        "                  if not isinstance(comment, praw.models.MoreComments)]\n",
        "\n",
        "      for comment in comments:\n",
        "        sentences += matcher.findall(comment)\n",
        "\n",
        "      len_sentences = len(sentences)\n",
        "      if len_sentences >= sentence_limit:\n",
        "        random.shuffle(sentences)\n",
        "        pbar.update(sentence_limit - pbar.n)\n",
        "        break\n",
        "      else:\n",
        "        pbar.update(len_sentences - pbar.n)\n",
        "  \n",
        "  return [process_sentence(sentence) for sentence in sentences[:sentence_limit]]\n",
        "\n",
        "\n",
        "def make_models(sentence_data):\n",
        "  \"\"\"Build markov chain models from extracted sentences\"\"\"\n",
        "\n",
        "  models = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
        "\n",
        "  for subreddit_name, sentences in sentence_data.items():\n",
        "    for sentence in sentences[:int(TRAIN_SPLIT*len(sentences))]:\n",
        "      words = split_sentence(sentence)\n",
        "      models[subreddit_name][BEGIN_STR][words[0]] += 1\n",
        "      models[subreddit_name][words[-1]][END_STR] += 1\n",
        "      for i in range(len(words) - 1):\n",
        "        models[subreddit_name][words[i]][words[i + 1]] += 1\n",
        "\n",
        "  # for subreddit_name, model in models.items():\n",
        "  #   for word1, counts in model.items():\n",
        "  #     sum_counts = sum(counts.values())\n",
        "  #     for word2 in counts:\n",
        "  #       models[subreddit_name][word1][word2] /= sum_counts\n",
        "\n",
        "  return models\n",
        "\n",
        "\n",
        "def generate(models, model_name, method):\n",
        "  \"\"\"Generate text using the created markov chain models\n",
        "\n",
        "  method:\n",
        "    expected: choose most likely words, infinite cycles are possible\n",
        "    random: choose words uniformly\n",
        "    sample: choose words based on the modeled probabilities\n",
        "  \"\"\"\n",
        "\n",
        "  sentence = \"\"\n",
        "  word = BEGIN_STR\n",
        "\n",
        "  if method == \"expected\":\n",
        "    used = set()\n",
        "\n",
        "  while True:\n",
        "    if method == \"expected\":\n",
        "      word = max(models[model_name][word].items(), key=lambda x: x[1])[0]\n",
        "    elif method == \"random\":\n",
        "      word = random.choice(tuple(models[model_name][word].items()))[0]\n",
        "    elif method == \"sample\":\n",
        "      words = tuple(models[model_name][word].keys())\n",
        "      probs = get_probs(models[model_name][word])\n",
        "      word = np.random.choice(words, p=probs)\n",
        "    if word == END_STR:\n",
        "      break\n",
        "    if word not in \".?!,\":\n",
        "      sentence += \" \"\n",
        "    sentence += word\n",
        "\n",
        "    if method == \"expected\":\n",
        "      if word in used:\n",
        "        sentence += f\" {CYCLE_STR}\"\n",
        "        break\n",
        "      used.add(word)\n",
        "\n",
        "  return sentence\n",
        "\n",
        "def classify(models, sentence):\n",
        "  \"\"\"Deduce the most likely source of a sentence\"\"\"\n",
        "  result = {}\n",
        "  for subreddit_name, model in models.items():\n",
        "    words = split_sentence(sentence)\n",
        "    p = get_prob(model[BEGIN_STR], words[0])\n",
        "    for i in range(len(words)-1):\n",
        "      p *= get_prob(model[words[i]], words[i+1])\n",
        "    p *= get_prob(model[words[-1]], END_STR)\n",
        "    result[subreddit_name] = p\n",
        "  return result\n",
        "\n",
        "\n",
        "def get_prob(d, word):\n",
        "  \"\"\"Get single probability from word counts\"\"\"\n",
        "  return 0 if word not in d else d[word]/sum(d.values())\n",
        "\n",
        "\n",
        "def get_probs(d):\n",
        "  \"\"\"Get all probabilities from word counts\"\"\"\n",
        "  n = sum(d.values())\n",
        "  return [v/n for v in d.values()]\n",
        "\n",
        "\n",
        "# def traverse_comments(comments, *, breadth_first=False):\n",
        "#   queue = deque(comments[:])\n",
        "#   result = []\n",
        "#   while queue:\n",
        "#     e = queue.pop()\n",
        "#     if isinstance(e, praw.models.MoreComments):\n",
        "#       if breadth_first:\n",
        "#         queue.extendleft(e.comments())\n",
        "#       else:\n",
        "#         queue.extend(e.comments())\n",
        "#     else:\n",
        "#       if breadth_first:\n",
        "#         queue.extendleft(e.replies)\n",
        "#       else:\n",
        "#         queue.extend(e.replies)\n",
        "#       result.append(e)\n",
        "#   return result"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD3OnLbKEvfF",
        "colab_type": "text"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGoyfE3idaXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d4cb8551-0ba6-4e34-8b08-640a9ddc7a41"
      },
      "source": [
        "sentence_data = {}\n",
        "for subreddit in SUBREDDITS:\n",
        "  sentence_data[subreddit] = mine_subreddit(\n",
        "      subreddit=reddit.subreddit(subreddit),\n",
        "      sentence_limit=SENTENCE_LIMIT)\n",
        "  \n",
        "models = make_models(sentence_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:16<00:00, 61.15it/s]\n",
            "100%|██████████| 1000/1000 [00:12<00:00, 79.11it/s]\n",
            "100%|██████████| 1000/1000 [00:23<00:00, 41.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRVVf74Jpq7r",
        "colab_type": "text"
      },
      "source": [
        "**Deriving most probable sentence for each model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqMPhSKlY2UG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dd7520e4-58ba-4572-e886-9916f2b230eb"
      },
      "source": [
        "for subreddit_name in SUBREDDITS:\n",
        "  print(f\"{subreddit_name}: {generate(models, subreddit_name, 'expected')}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "explainlikeimfive:  I am not know about the same way to the *CYCLE*\n",
            "askreddit:  I am a lot of the first time.\n",
            "dankmemes:  I am not a lot of the other.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-kULdBpY6Z",
        "colab_type": "text"
      },
      "source": [
        "**Generating new text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDInUB-pnBSY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "878ab2c4-3b43-451c-eee2-28337ec19ff9"
      },
      "source": [
        "dict_data = defaultdict(lambda: [])\n",
        "for subreddit_name, model in models.items():\n",
        "  for i in range(5):\n",
        "    sentence = (generate(models, subreddit_name, \"sample\"))\n",
        "    dict_data[\"sentence\"].append(sentence)\n",
        "    dict_data[\"model\"].append(subreddit_name)\n",
        "    res = classify(models, sentence)\n",
        "    for k, v in res.items():\n",
        "      dict_data[f\"P({k})\"].append(v)\n",
        "display(pd.DataFrame(dict_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>model</th>\n",
              "      <th>P(explainlikeimfive)</th>\n",
              "      <th>P(askreddit)</th>\n",
              "      <th>P(dankmemes)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which means your desired result is a quarter of objects that gives you have to believe the atmosphere it because you will group New World, or so will glare at 350 was a philosophical movement that way in Oceania but in facilitating financial abilities.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>5.254677e-52</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Greece, the US to the title and decide what other factors determine that are Central European.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>3.373439e-22</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>While I used to carbon dating, Czechs, how it possible for example, European.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>5.251718e-23</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I shall leave the Pacific Ocean is an awful experience?</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>9.834314e-11</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They can warm yellow tones lead to be half the food is super simple puzzles have been that touches it.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>5.946787e-21</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Golden deer currently.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.555556e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Otherwise the first time it seems to care less in the usual suspects, I dare you not trust anyone who lived down.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.561821e-27</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Considering it's own weight.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.469136e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Played CTR as often the GOP care less than on the afternoon.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.843878e-14</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>The 7pm cheering for memes because deep down so abortion rights to both on Earth.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.331422e-16</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Using a comma makes it too!</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.499081e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Sadly it's from my mind boggingly gorgeous but does we actively trying to go.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.222650e-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Hahaha.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.111111e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Oh, its a stupid.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.698129e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Came here looking for him.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.703704e-06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                         sentence  ...  P(dankmemes)\n",
              "0    Which means your desired result is a quarter of objects that gives you have to believe the atmosphere it because you will group New World, or so will glare at 350 was a philosophical movement that way in Oceania but in facilitating financial abilities.  ...  0.000000e+00\n",
              "1                                                                                                                                                                  Greece, the US to the title and decide what other factors determine that are Central European.  ...  0.000000e+00\n",
              "2                                                                                                                                                                                   While I used to carbon dating, Czechs, how it possible for example, European.  ...  0.000000e+00\n",
              "3                                                                                                                                                                                                         I shall leave the Pacific Ocean is an awful experience?  ...  0.000000e+00\n",
              "4                                                                                                                                                          They can warm yellow tones lead to be half the food is super simple puzzles have been that touches it.  ...  0.000000e+00\n",
              "5                                                                                                                                                                                                                                          Golden deer currently.  ...  0.000000e+00\n",
              "6                                                                                                                                               Otherwise the first time it seems to care less in the usual suspects, I dare you not trust anyone who lived down.  ...  0.000000e+00\n",
              "7                                                                                                                                                                                                                                    Considering it's own weight.  ...  0.000000e+00\n",
              "8                                                                                                                                                                                                    Played CTR as often the GOP care less than on the afternoon.  ...  0.000000e+00\n",
              "9                                                                                                                                                                               The 7pm cheering for memes because deep down so abortion rights to both on Earth.  ...  0.000000e+00\n",
              "10                                                                                                                                                                                                                                    Using a comma makes it too!  ...  1.499081e-09\n",
              "11                                                                                                                                                                                  Sadly it's from my mind boggingly gorgeous but does we actively trying to go.  ...  4.222650e-15\n",
              "12                                                                                                                                                                                                                                                        Hahaha.  ...  1.111111e-03\n",
              "13                                                                                                                                                                                                                                              Oh, its a stupid.  ...  1.698129e-08\n",
              "14                                                                                                                                                                                                                                     Came here looking for him.  ...  3.703704e-06\n",
              "\n",
              "[15 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMyrKXKJpl_J",
        "colab_type": "text"
      },
      "source": [
        "**Classifying real text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J92CBICZnTrg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "1cf03387-cc4d-4d2d-aa7c-347e8b9503a3"
      },
      "source": [
        "dict_data = defaultdict(lambda: [])\n",
        "for subreddit_name, sentences in sentence_data.items():\n",
        "  for i in tuple(range(int(TEST_SPLIT*len(sentences))))[:5]:\n",
        "    sentence = sentences[int(TRAIN_SPLIT*len(sentences)) + i]\n",
        "    dict_data[\"sentence\"].append(sentence)\n",
        "    dict_data[\"source\"].append(subreddit_name)\n",
        "    res = classify(models, sentence)\n",
        "    for k, v in res.items():\n",
        "      dict_data[f\"P({k})\"].append(v)\n",
        "display(pd.DataFrame(dict_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>source</th>\n",
              "      <th>P(explainlikeimfive)</th>\n",
              "      <th>P(askreddit)</th>\n",
              "      <th>P(dankmemes)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Latin America is usually referred to as Latin America, The Global South, or developing countries.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Feels good after having a meal so cheap and not unhealthy.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It got used so regularly in our house that it just always sat on the counter.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>However, when a seed sprouts, lots of changed occur in the building blocks that make up the seed, which let it to be able to grow.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Perhaps Orthodox Christianity plays a role?</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HONK.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I do not even understand his excuse why it's okay this year and not in the previous election year, what does he mean it's only inappropriate if two different parties are being represented this year are not there two parties being represented on the ballot Republicans and Democrats, am I having a brain fart?</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I still have not rewatched that movie as the reveal was so epic I knew I could never feel that way about it again.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>However, I've noticed that lots of the civilians in NY do not get that emotional over politics, they are mostly level headed.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>In the middle of a pandemic you do not get much more win than that.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Women should be less scared of lesbians than men, due to their diminished sex drive and strength.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>They see dirty politicians, they see their lives getting worse and worse, and if someone is in a bad enough place they will take anything that could change it.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Wait till you see my India taking number 1.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Take my unreal gold.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>I am quite literally not wrong, but ok.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                sentence  ... P(dankmemes)\n",
              "0                                                                                                                                                                                                                      Latin America is usually referred to as Latin America, The Global South, or developing countries.  ...          0.0\n",
              "1                                                                                                                                                                                                                                                             Feels good after having a meal so cheap and not unhealthy.  ...          0.0\n",
              "2                                                                                                                                                                                                                                          It got used so regularly in our house that it just always sat on the counter.  ...          0.0\n",
              "3                                                                                                                                                                                     However, when a seed sprouts, lots of changed occur in the building blocks that make up the seed, which let it to be able to grow.  ...          0.0\n",
              "4                                                                                                                                                                                                                                                                            Perhaps Orthodox Christianity plays a role?  ...          0.0\n",
              "5                                                                                                                                                                                                                                                                                                                  HONK.  ...          0.0\n",
              "6   I do not even understand his excuse why it's okay this year and not in the previous election year, what does he mean it's only inappropriate if two different parties are being represented this year are not there two parties being represented on the ballot Republicans and Democrats, am I having a brain fart?  ...          0.0\n",
              "7                                                                                                                                                                                                     I still have not rewatched that movie as the reveal was so epic I knew I could never feel that way about it again.  ...          0.0\n",
              "8                                                                                                                                                                                          However, I've noticed that lots of the civilians in NY do not get that emotional over politics, they are mostly level headed.  ...          0.0\n",
              "9                                                                                                                                                                                                                                                    In the middle of a pandemic you do not get much more win than that.  ...          0.0\n",
              "10                                                                                                                                                                                                                     Women should be less scared of lesbians than men, due to their diminished sex drive and strength.  ...          0.0\n",
              "11                                                                                                                                                       They see dirty politicians, they see their lives getting worse and worse, and if someone is in a bad enough place they will take anything that could change it.  ...          0.0\n",
              "12                                                                                                                                                                                                                                                                           Wait till you see my India taking number 1.  ...          0.0\n",
              "13                                                                                                                                                                                                                                                                                                  Take my unreal gold.  ...          0.0\n",
              "14                                                                                                                                                                                                                                                                               I am quite literally not wrong, but ok.  ...          0.0\n",
              "\n",
              "[15 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqmvWFGHFEY9",
        "colab_type": "text"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AaPLFDCFFun",
        "colab_type": "text"
      },
      "source": [
        "*   Second order markov chains: P(AB->C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqPVLY7jEIW1",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvTiyWstD104",
        "colab_type": "text"
      },
      "source": [
        "*   https://en.wikipedia.org/wiki/Markov_chain\n",
        "*   https://www.reddit.com/r/SubredditSimulator/comments/3g9ioz/what_is_rsubredditsimulator/\n",
        "*   https://www.reddit.com/r/SubSimulatorGPT2/comments/btfhks/what_is_rsubsimulatorgpt2/"
      ]
    }
  ]
}
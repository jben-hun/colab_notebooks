{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "markov_sentences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOE9hfG5hY4toLnyL8P7Db",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jben-hun/colab_notebooks/blob/master/algorithms/markov_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEw4MXYzEpwO"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDh1P8w8bjPh",
        "outputId": "49053096-92a0-4dc1-8980-f87a63d6aa61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install -q praw\n",
        "\n",
        "import praw\n",
        "import re\n",
        "import random\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from collections import deque\n",
        "\n",
        "pd.set_option(\"max_colwidth\", None)\n",
        "\n",
        "client_id = \"\" #@param {type:\"string\"}\n",
        "client_secret = \"\" #@param {type:\"string\"}\n",
        "user_agent = \"\" #@param {type:\"string\"}\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=client_id,\n",
        "    client_secret=client_secret,\n",
        "    user_agent=user_agent)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 153kB 4.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 7.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWUQX6EAKptQ"
      },
      "source": [
        "SUBREDDITS = (\"askreddit\", \"explainlikeimfive\", \"dankmemes\")\n",
        "\n",
        "\n",
        "class RedditMarkovChain:\n",
        "    def __init__(\n",
        "            self,\n",
        "            subreddit,\n",
        "            sentence_limit=1000,\n",
        "            begin_str=\"*BEGIN*\",\n",
        "            end_str=\"*END*\",\n",
        "            cycle_str=\"*CYCLE*\",\n",
        "            train_split=(0.9)):\n",
        "        self.__subreddit = subreddit\n",
        "        self.__sentence_limit = sentence_limit\n",
        "        self.__begin_str = begin_str\n",
        "        self.__end_str = end_str\n",
        "        self.__cycle_str = cycle_str\n",
        "        self.__train_split = train_split\n",
        "        self.__test_split = (1.0 - train_split)\n",
        "\n",
        "        self.__sentences = self.mine_subreddit(\n",
        "            subreddit=reddit.subreddit(self.subreddit),\n",
        "            sentence_limit=self.sentence_limit)\n",
        "\n",
        "        self.model = self.__build_model()\n",
        "\n",
        "    def __build_model(self):\n",
        "        \"\"\"Build a markov chain model from extracted sentences\"\"\"\n",
        "        model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "        for sentence in self.train_sentences:\n",
        "            words = self.split_sentence(sentence)\n",
        "            model[self.begin_str][words[0]] += 1\n",
        "            model[words[-1]][self.end_str] += 1\n",
        "            for i in range(len(words) - 1):\n",
        "                model[words[i]][words[i + 1]] += 1\n",
        "\n",
        "        return model\n",
        "\n",
        "    def generate(self, method=\"sample\"):\n",
        "        \"\"\"Generate text using the created markov chain model\n",
        "\n",
        "        method:\n",
        "            expected: choose most likely words, infinite cycles are possible\n",
        "            random: choose words uniformly\n",
        "            sample: choose words based on the modeled probabilities\n",
        "        \"\"\"\n",
        "\n",
        "        sentence = \"\"\n",
        "        word = self.begin_str\n",
        "\n",
        "        if method == \"expected\":\n",
        "            used = set()\n",
        "\n",
        "        while True:\n",
        "            if method == \"expected\":\n",
        "                word = max(\n",
        "                    self.model[word].items(), key=lambda x: x[1])[0]\n",
        "            elif method == \"random\":\n",
        "                word = random.choice(tuple(self.model[word].items()))[0]\n",
        "            elif method == \"sample\":\n",
        "                words = tuple(self.model[word].keys())\n",
        "                probs = self.get_probs(self.model[word])\n",
        "                word = np.random.choice(words, p=probs)\n",
        "            if word == self.end_str:\n",
        "                break\n",
        "            if word not in \".?!,\":\n",
        "                sentence += \" \"\n",
        "            sentence += word\n",
        "\n",
        "            if method == \"expected\":\n",
        "                if word in used:\n",
        "                    sentence += f\" {self.cycle_str}\"\n",
        "                    break\n",
        "                used.add(word)\n",
        "\n",
        "        return sentence\n",
        "\n",
        "    def classify(self, sentence):\n",
        "        \"\"\"Deduce the most likely source of a sentence\"\"\"\n",
        "\n",
        "        words = self.split_sentence(sentence)\n",
        "\n",
        "        p = self.get_prob(self.model[self.begin_str], words[0])\n",
        "        for i in range(len(words)-1):\n",
        "            p *= self.get_prob(self.model[words[i]], words[i+1])\n",
        "        p *= self.get_prob(self.model[words[-1]], self.end_str)\n",
        "\n",
        "        return p\n",
        "\n",
        "    @classmethod\n",
        "    def mine_subreddit(cls, subreddit, sentence_limit):\n",
        "        \"\"\"Extract clean sentences from submissions and comments\"\"\"\n",
        "\n",
        "        # re that matches clean sentences\n",
        "        matcher = re.compile(r\"(?:[.!?] |^)[A-Z][\\w', ]+[.!?](?= [A-Z]|$)\")\n",
        "\n",
        "        sentences = []\n",
        "\n",
        "        with tqdm.tqdm(total=sentence_limit) as pbar:\n",
        "            for submission in subreddit.hot(limit=None):\n",
        "                sentences += matcher.findall(submission.title)\n",
        "                sentences += matcher.findall(submission.selftext)\n",
        "\n",
        "                submission.comment_sort = \"best\"\n",
        "\n",
        "                comments = [\n",
        "                    comment.body for comment in submission.comments.list()\n",
        "                    if not isinstance(comment, praw.models.MoreComments)]\n",
        "\n",
        "                for comment in comments:\n",
        "                    sentences += matcher.findall(comment)\n",
        "\n",
        "                len_sentences = len(sentences)\n",
        "                if len_sentences >= sentence_limit:\n",
        "                    random.shuffle(sentences)\n",
        "                    pbar.update(sentence_limit - pbar.n)\n",
        "                    break\n",
        "                else:\n",
        "                    pbar.update(len_sentences - pbar.n)\n",
        "\n",
        "        return [cls.process_sentence(sentence) for sentence\n",
        "                in sentences[:sentence_limit]]\n",
        "\n",
        "    @staticmethod\n",
        "    def process_sentence(sentence):\n",
        "        \"\"\"Clean up sentences\"\"\"\n",
        "        return (sentence.lstrip(\".!? \")\n",
        "                        .replace(\"won't\", \"will not\")\n",
        "                        .replace(\"n't\", \" not\")\n",
        "                        .replace(\"'m\", \" am\")\n",
        "                        .replace(\"'re\", \" are\"))\n",
        "\n",
        "    @staticmethod\n",
        "    def split_sentence(sentence):\n",
        "        \"\"\"Split sentences into words\"\"\"\n",
        "        return re.findall(r\"((?:[\\w']+)|(?:[,!.?]))\", sentence)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_prob(d, word):\n",
        "        \"\"\"Get single probability from word counts\"\"\"\n",
        "        return 0 if word not in d else d[word]/sum(d.values())\n",
        "\n",
        "    @staticmethod\n",
        "    def get_probs(d):\n",
        "        \"\"\"Get all probabilities from word counts\"\"\"\n",
        "        n = sum(d.values())\n",
        "        return [v/n for v in d.values()]\n",
        "\n",
        "    @staticmethod\n",
        "    def traverse_comments(comments, *, breadth_first=False):\n",
        "        queue = deque(comments[:])\n",
        "        result = []\n",
        "        while queue:\n",
        "            e = queue.pop()\n",
        "            if isinstance(e, praw.models.MoreComments):\n",
        "                if breadth_first:\n",
        "                    queue.extendleft(e.comments())\n",
        "                else:\n",
        "                    queue.extend(e.comments())\n",
        "            else:\n",
        "                if breadth_first:\n",
        "                    queue.extendleft(e.replies)\n",
        "                else:\n",
        "                    queue.extend(e.replies)\n",
        "                result.append(e)\n",
        "        return result\n",
        "\n",
        "    @property\n",
        "    def subreddit(self):\n",
        "        return self.__subreddit\n",
        "\n",
        "    @property\n",
        "    def sentences(self):\n",
        "        return self.__sentences\n",
        "\n",
        "    @property\n",
        "    def sentence_limit(self):\n",
        "        return self.__sentence_limit\n",
        "\n",
        "    @property\n",
        "    def begin_str(self):\n",
        "        return self.__begin_str\n",
        "\n",
        "    @property\n",
        "    def end_str(self):\n",
        "        return self.__end_str\n",
        "\n",
        "    @property\n",
        "    def cycle_str(self):\n",
        "        return self.__cycle_str\n",
        "\n",
        "    @property\n",
        "    def train_split(self):\n",
        "        return self.__train_split\n",
        "\n",
        "    @property\n",
        "    def test_split(self):\n",
        "        return self.__test_split\n",
        "\n",
        "    @property\n",
        "    def train_sentences(self):\n",
        "        return self.sentences[:int(len(self.sentences)*self.train_split)]\n",
        "\n",
        "    @property\n",
        "    def test_sentences(self):\n",
        "        return self.sentences[int(len(self.sentences)*self.train_split):]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD3OnLbKEvfF"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGoyfE3idaXk",
        "outputId": "f92119e3-e084-4027-fcfb-9a614d159bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "chains = {subreddit: RedditMarkovChain(subreddit, sentence_limit=1000)\n",
        "          for subreddit in SUBREDDITS}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:12<00:00, 77.16it/s]\n",
            "100%|██████████| 1000/1000 [00:19<00:00, 51.15it/s]\n",
            "100%|██████████| 1000/1000 [00:32<00:00, 30.90it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRVVf74Jpq7r"
      },
      "source": [
        "**Deriving most probable sentence for each model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqMPhSKlY2UG",
        "outputId": "8b780e47-cdc5-4213-eca9-aafbc05cbb53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "for subreddit, chain in chains.items():\n",
        "    print(f\"{subreddit}: {chain.generate('expected')}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "askreddit:  I was a kid.\n",
            "explainlikeimfive:  I think it's not a lot of the same with the *CYCLE*\n",
            "dankmemes:  I am not the imposter.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-kULdBpY6Z"
      },
      "source": [
        "**Generating new text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDInUB-pnBSY",
        "outputId": "7c03fcf0-1d41-4306-8e4c-a14608d0aabe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "dict_data = defaultdict(lambda: [])\n",
        "\n",
        "for subreddit, chain in chains.items():\n",
        "    for _ in range(5):\n",
        "        sentence = chain.generate()\n",
        "        dict_data[\"sentence\"].append(sentence)\n",
        "        dict_data[\"model\"].append(subreddit)\n",
        "\n",
        "        for k, v in chains.items():\n",
        "            p = v.classify(sentence)\n",
        "            dict_data[f\"P({k})\"].append(p)\n",
        "\n",
        "display(pd.DataFrame(dict_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>model</th>\n",
              "      <th>P(askreddit)</th>\n",
              "      <th>P(explainlikeimfive)</th>\n",
              "      <th>P(dankmemes)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>They could be a year, his entire tenure on by the battle with the Justice, I believed.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>8.500275e-26</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One does not stop calling to it is a kid.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>4.529957e-13</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I mean, Roe v Casey is such a liberal.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>4.942273e-14</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Yup, I am not a few weeks into my high school sent to get representation in my age.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>3.556284e-22</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>One does the presidency so often.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>2.909887e-09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Can you.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>9.132420e-05</td>\n",
              "      <td>9.876543e-05</td>\n",
              "      <td>1.769912e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>So kernel itself is pretty simple explanation friend.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.293123e-10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>A 16 year old human babies I think it's making the game.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8.364851e-13</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>And the right order to being in momentum.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.239369e-12</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>This is easier target.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.089325e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Maybe it was still better.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.321973e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>It's an image get it was so much and I am found in 2017.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.131590e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>You fucking liars all to make a japanese student?</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.152247e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>You shave a meme quality and called out of the first time?</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.516538e-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Are you?</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>6.088280e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.474926e-04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                   sentence  ...  P(dankmemes)\n",
              "0    They could be a year, his entire tenure on by the battle with the Justice, I believed.  ...  0.000000e+00\n",
              "1                                                 One does not stop calling to it is a kid.  ...  0.000000e+00\n",
              "2                                                    I mean, Roe v Casey is such a liberal.  ...  0.000000e+00\n",
              "3       Yup, I am not a few weeks into my high school sent to get representation in my age.  ...  0.000000e+00\n",
              "4                                                         One does the presidency so often.  ...  0.000000e+00\n",
              "5                                                                                  Can you.  ...  1.769912e-04\n",
              "6                                     So kernel itself is pretty simple explanation friend.  ...  0.000000e+00\n",
              "7                                  A 16 year old human babies I think it's making the game.  ...  0.000000e+00\n",
              "8                                                 And the right order to being in momentum.  ...  0.000000e+00\n",
              "9                                                                    This is easier target.  ...  0.000000e+00\n",
              "10                                                               Maybe it was still better.  ...  6.321973e-08\n",
              "11                                 It's an image get it was so much and I am found in 2017.  ...  1.131590e-16\n",
              "12                                        You fucking liars all to make a japanese student?  ...  5.152247e-11\n",
              "13                               You shave a meme quality and called out of the first time?  ...  5.516538e-15\n",
              "14                                                                                 Are you?  ...  1.474926e-04\n",
              "\n",
              "[15 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMyrKXKJpl_J"
      },
      "source": [
        "**Classifying real text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J92CBICZnTrg",
        "outputId": "224b9cbe-0c05-4c76-806e-8eff10cd45c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "dict_data = defaultdict(lambda: [])\n",
        "\n",
        "for subreddit, chain in chains.items():\n",
        "    for sentence in chain.test_sentences[:5]:\n",
        "        dict_data[\"sentence\"].append(sentence)\n",
        "        dict_data[\"source\"].append(subreddit)\n",
        "\n",
        "        for k, v in chains.items():\n",
        "            p = v.classify(sentence)\n",
        "            dict_data[f\"P({k})\"].append(p)\n",
        "\n",
        "display(pd.DataFrame(dict_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>source</th>\n",
              "      <th>P(askreddit)</th>\n",
              "      <th>P(explainlikeimfive)</th>\n",
              "      <th>P(dankmemes)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I think the death of Artax is still one of the most heartbreaking moments in movie history!</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'd still call it a beautiful friendship.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cool Runnings.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We are fucked even if he is not.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I always get the feeling that people who post this type of stuff are lying just to get attention and karma.</td>\n",
              "      <td>askreddit</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>You are correct, a wider base of support is more stable.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>You should check it out!</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Gaseous compounds naturally want to fill the area they are in equally, so that each molecule is an equal distance away from each other molecule.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I have had pets that I do not immeditaely love.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Some people, very much do, have an emotional connection with their car.</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Wait are those actual tiger 2s in service ?</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>You had me at spaghetti.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Amen.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>What?</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>I am just following the rules the mods put in place.</td>\n",
              "      <td>dankmemes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                            sentence  ... P(dankmemes)\n",
              "0                                                        I think the death of Artax is still one of the most heartbreaking moments in movie history!  ...     0.000000\n",
              "1                                                                                                          I'd still call it a beautiful friendship.  ...     0.000000\n",
              "2                                                                                                                                     Cool Runnings.  ...     0.000000\n",
              "3                                                                                                                   We are fucked even if he is not.  ...     0.000000\n",
              "4                                        I always get the feeling that people who post this type of stuff are lying just to get attention and karma.  ...     0.000000\n",
              "5                                                                                           You are correct, a wider base of support is more stable.  ...     0.000000\n",
              "6                                                                                                                           You should check it out!  ...     0.000000\n",
              "7   Gaseous compounds naturally want to fill the area they are in equally, so that each molecule is an equal distance away from each other molecule.  ...     0.000000\n",
              "8                                                                                                    I have had pets that I do not immeditaely love.  ...     0.000000\n",
              "9                                                                            Some people, very much do, have an emotional connection with their car.  ...     0.000000\n",
              "10                                                                                                       Wait are those actual tiger 2s in service ?  ...     0.000000\n",
              "11                                                                                                                          You had me at spaghetti.  ...     0.000000\n",
              "12                                                                                                                                             Amen.  ...     0.000000\n",
              "13                                                                                                                                             What?  ...     0.001111\n",
              "14                                                                                              I am just following the rules the mods put in place.  ...     0.000000\n",
              "\n",
              "[15 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqmvWFGHFEY9"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AaPLFDCFFun"
      },
      "source": [
        "*   Second order markov chains: P(AB->C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqPVLY7jEIW1"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvTiyWstD104"
      },
      "source": [
        "*   https://en.wikipedia.org/wiki/Markov_chain\n",
        "*   https://www.reddit.com/r/SubredditSimulator/comments/3g9ioz/what_is_rsubredditsimulator/\n",
        "*   https://www.reddit.com/r/SubSimulatorGPT2/comments/btfhks/what_is_rsubsimulatorgpt2/"
      ]
    }
  ]
}
